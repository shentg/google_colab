{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shentg/google_colab/blob/master/transformers_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0trJmd6DjqBZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c91f4deb-230a-46e5-fd93-8c19ce39f515"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcK92vqUOSoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNNQCy2irs-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9dd55371-24d2-4fd9-c4e8-8c4568d57731"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  # with tf.Session(tpu_address) as session:\n",
        "  #   devices = session.list_devices()\n",
        "\n",
        "  # print('TPU devices:')\n",
        "  # pprint.pprint(devices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Not connected to a TPU runtime\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMTnotnVviT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4275fd67-b1c6-4f56-a38d-ddea88e2f8ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqcErXp9GNrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d781fd95-c8a0-4351-f0af-b9d01f845549"
      },
      "source": [
        "%cd /content/gdrive/My Drive/transformers\n",
        "import os\n",
        "from getpass import getpass\n",
        "user = getpass('GitHub user')\n",
        "password = getpass('GitHub password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "!rm -rf vadernlp\n",
        "!git clone https://$GITHUB_AUTH@github.com/shentg/vadernlp.git\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/transformers\n",
            "GitHub user··········\n",
            "GitHub password··········\n",
            "Cloning into 'vadernlp'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 113 (delta 39), reused 112 (delta 38), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (113/113), 4.57 MiB | 3.45 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djGLLaQ0vHCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "88ef37f4-2e80-42ee-bb07-345ad73406d6"
      },
      "source": [
        "%cd /content/gdrive/My Drive/transformers/vadernlp"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/transformers/vadernlp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jGnGthM66tQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3908a20c-f5a2-48fa-d076-552d418bccd4"
      },
      "source": [
        "!echo $PYTHONPATH\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/gdrive/My Drive/transformers/vadernlp/\"\n",
        "!echo $PYTHONPATH"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6:/env/python\n",
            "/tensorflow-2.1.0/python3.6:/env/python:/content/gdrive/My Drive/transformers/vadernlp/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASfx0dWzuOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18b27449-922b-4969-8d86-64ce12a6f135"
      },
      "source": [
        "!python3 bin/ner.py --data_dir=/content/gdrive/My\\ Drive/transformers/vadernlp/data \\\n",
        "--model_type=bert \\\n",
        "--model_name_or_path=bert-base-chinese \\\n",
        "--do_train=True \\\n",
        "--do_eval=True \\\n",
        "--do_predict=True \\\n",
        "--output_dir=/content/gdrive/My\\ Drive/transformers/vadernlp/output \\\n",
        "--fp16=False \\\n",
        "--overwrite_output_dir=True \\\n",
        "--save_steps=10000 \\\n",
        "--logging_steps=10000 \\\n",
        "--labels=/content/gdrive/My\\ Drive/transformers/vadernlp/data/labels.txt \\\n",
        "--learning_rate=5e-5 \\\n",
        "# --tpu='grpc://10.115.20.154:8470'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
            "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n",
            "W0103 11:47:44.597607 140370661570432 ner.py:502] n_device: 1, distributed training: False, 16-bits training: False\n",
            "I0103 11:47:45.450981 140370661570432 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
            "I0103 11:47:45.451339 140370661570432 configuration_utils.py:199] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "I0103 11:47:45.452035 140370661570432 ner.py:515] Training/evaluation parameters {'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'info', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'data_dir': '/content/gdrive/My Drive/transformers/vadernlp/data', 'model_type': 'bert', 'model_name_or_path': 'bert-base-chinese', 'output_dir': '/content/gdrive/My Drive/transformers/vadernlp/output', 'labels': '/content/gdrive/My Drive/transformers/vadernlp/data/labels.txt', 'config_name': '', 'tokenizer_name': '', 'cache_dir': '', 'max_seq_length': 128, 'tpu': None, 'num_tpu_cores': 8, 'do_train': True, 'do_eval': True, 'do_predict': True, 'evaluate_during_training': False, 'do_lower_case': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3, 'max_steps': -1, 'warmup_steps': 0, 'logging_steps': 10000, 'save_steps': 10000, 'eval_all_checkpoints': False, 'no_cuda': False, 'overwrite_output_dir': True, 'overwrite_cache': False, 'seed': 42, 'fp16': False, 'gpus': '0', '?': False, 'help': False, 'helpshort': False, 'helpfull': False, 'helpxml': False, 'n_device': 1}\n",
            "I0103 11:47:46.197707 140370661570432 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
            "I0103 11:47:46.973315 140370661570432 modeling_tf_utils.py:300] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-tf_model.h5 from cache at /root/.cache/torch/transformers/987cd265ea1aa9cd7e884caf8dd86c2e764e5114ee9a14a67686c1fe05f7a26c.e6b974f59b54219496a89fd32be7afb020374df0976a796e5ccd3a1733d31537.h5\n",
            "2020-01-03 11:47:47.211013: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0103 11:47:49.238844 140370661570432 modeling_tf_utils.py:336] Layers of TFBertForTokenClassification not initialized from pretrained model: ['classifier', 'dropout_37']\n",
            "I0103 11:47:49.239051 140370661570432 modeling_tf_utils.py:339] Layers from pretrained model not used in TFBertForTokenClassification: ['nsp___cls', 'mlm___cls']\n",
            "I0103 11:47:49.239890 140370661570432 ner.py:425] Loading features from cached file /content/gdrive/My Drive/transformers/vadernlp/data/cached_train_bert-base-chinese_128.tf_record\n",
            "I0103 11:47:49.449688 140370661570432 ner.py:175] ***** Running training *****\n",
            "I0103 11:47:49.449860 140370661570432 ner.py:176]   Num examples = 0\n",
            "I0103 11:47:49.450302 140370661570432 ner.py:177]   Num Epochs = 3\n",
            "I0103 11:47:49.450458 140370661570432 ner.py:178]   Instantaneous batch size per device = 8\n",
            "I0103 11:47:49.450549 140370661570432 ner.py:181]   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "I0103 11:47:49.450626 140370661570432 ner.py:183]   Gradient Accumulation steps = 1\n",
            "I0103 11:47:49.450700 140370661570432 ner.py:184]   Total training steps = 0\n",
            "Model: \"tf_bert_for_token_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  102267648 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 102,269,186\n",
            "Trainable params: 102,269,186\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "/usr/local/lib/python3.6/dist-packages/fastprogress/fastprogress.py:105: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "<IPython.core.display.HTML object>\n",
            "I0103 11:47:49.653990 140370661570432 ner.py:308]   Training took time = 0:00:00.180327\n",
            "I0103 11:47:49.654945 140370661570432 ner.py:554] Saving model to /content/gdrive/My Drive/transformers/vadernlp/output\n",
            "I0103 11:47:49.667248 140370661570432 configuration_utils.py:87] Configuration saved in /content/gdrive/My Drive/transformers/vadernlp/output/config.json\n",
            "I0103 11:47:52.083174 140370661570432 modeling_tf_utils.py:162] Model weights saved in /content/gdrive/My Drive/transformers/vadernlp/output/tf_model.h5\n",
            "I0103 11:47:52.114238 140370661570432 tokenization_utils.py:327] Model name '/content/gdrive/My Drive/transformers/vadernlp/output' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/content/gdrive/My Drive/transformers/vadernlp/output' is a path or url to a directory containing tokenizer files.\n",
            "I0103 11:47:52.117071 140370661570432 tokenization_utils.py:395] loading file /content/gdrive/My Drive/transformers/vadernlp/output/vocab.txt\n",
            "I0103 11:47:52.117208 140370661570432 tokenization_utils.py:395] loading file /content/gdrive/My Drive/transformers/vadernlp/output/added_tokens.json\n",
            "I0103 11:47:52.117285 140370661570432 tokenization_utils.py:395] loading file /content/gdrive/My Drive/transformers/vadernlp/output/special_tokens_map.json\n",
            "I0103 11:47:52.117368 140370661570432 tokenization_utils.py:395] loading file /content/gdrive/My Drive/transformers/vadernlp/output/tokenizer_config.json\n",
            "I0103 11:47:52.827372 140370661570432 ner.py:574] Evaluate the following checkpoints: []\n",
            "I0103 11:47:52.829165 140370661570432 configuration_utils.py:182] loading configuration file /content/gdrive/My Drive/transformers/vadernlp/output/config.json\n",
            "I0103 11:47:52.829484 140370661570432 configuration_utils.py:199] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 21128\n",
            "}\n",
            "\n",
            "I0103 11:47:52.830057 140370661570432 modeling_tf_utils.py:297] loading weights file /content/gdrive/My Drive/transformers/vadernlp/output/tf_model.h5\n",
            "I0103 11:47:54.553121 140370661570432 modeling_tf_utils.py:336] Layers of TFBertForTokenClassification not initialized from pretrained model: ['dropout_75']\n",
            "I0103 11:47:54.553406 140370661570432 modeling_tf_utils.py:339] Layers from pretrained model not used in TFBertForTokenClassification: ['dropout_37']\n",
            "I0103 11:47:54.566273 140370661570432 ner.py:428] Creating features from dataset file at /content/gdrive/My Drive/transformers/vadernlp/data\n",
            "I0103 11:47:54.692107 140370661570432 utils_ner.py:107] Writing example 0 of 2318\n",
            "I0103 11:47:54.695050 140370661570432 utils_ner.py:183] *** Example ***\n",
            "I0103 11:47:54.695152 140370661570432 utils_ner.py:184] guid: dev-1\n",
            "I0103 11:47:54.695247 140370661570432 utils_ner.py:185] tokens: [CLS] 在 这 里 恕 弟 不 恭 之 罪 ， 敢 在 尊 前 一 诤 ： 前 人 论 书 ， 每 曰 [UNK] 字 字 有 来 历 ， 笔 笔 有 出 处 [UNK] ， 细 读 公 字 ， 何 尝 跳 出 前 人 藩 篱 ， 自 隶 变 而 后 ， 直 至 明 季 ， 兄 有 何 新 出 ？ [SEP]\n",
            "I0103 11:47:54.695400 140370661570432 utils_ner.py:186] input_ids: 101 1762 6821 7027 2609 2475 679 2621 722 5389 8024 3140 1762 2203 1184 671 6420 8038 1184 782 6389 741 8024 3680 3288 100 2099 2099 3300 3341 1325 8024 5011 5011 3300 1139 1905 100 8024 5301 6438 1062 2099 8024 862 2214 6663 1139 1184 782 5974 5075 8024 5632 7405 1359 5445 1400 8024 4684 5635 3209 2108 8024 1040 3300 862 3173 1139 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0103 11:47:54.695519 140370661570432 utils_ner.py:187] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0103 11:47:54.695616 140370661570432 utils_ner.py:188] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0103 11:47:54.695706 140370661570432 utils_ner.py:189] label_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "Traceback (most recent call last):\n",
            "  File \"bin/ner.py\", line 652, in <module>\n",
            "    app.run(main)\n",
            "  File \"/tensorflow-2.1.0/python3.6/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/tensorflow-2.1.0/python3.6/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"bin/ner.py\", line 587, in main\n",
            "    args, strategy, model, tokenizer, labels, pad_token_label_id, mode=\"dev\"\n",
            "  File \"bin/ner.py\", line 314, in evaluate\n",
            "    args, tokenizer, labels, pad_token_label_id, eval_batch_size, mode=mode\n",
            "  File \"bin/ner.py\", line 446, in load_and_cache_examples\n",
            "    pad_token_label_id=pad_token_label_id,\n",
            "  File \"/content/gdrive/My Drive/transformers/vadernlp/vadernlp/utils/utils_ner.py\", line 115, in convert_examples_to_features\n",
            "    label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
            "KeyError: 'B-ORG'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot_La6oKO05f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}